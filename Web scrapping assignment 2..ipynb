{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text = requests.get(\"https://in.indeed.com/jobs?q=data%20analyst&l=Bangalore%2C%20Karnataka\").text\n",
    "soup = BeautifulSoup(html_text,'html.parser')\n",
    "jobs = soup.find_all('div',class_='jobsearch-SerpJobCard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_name = []\n",
    "company = []\n",
    "summary = []\n",
    "job_location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for job in jobs:\n",
    "    atag = job.h2.a\n",
    "    job_title = atag.get('title')\n",
    "    job_name.append(job_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in jobs:\n",
    "    ctag = x.find('span','company').text.strip()\n",
    "    company.append(ctag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for location in jobs:\n",
    "    place = location.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    job_location.append(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in jobs:\n",
    "    sm = i.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    summary.append(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>EXPECTATIONS</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Analyst (1-2 Yrs)</td>\n",
       "      <td>CommerceIQ</td>\n",
       "      <td>data integration, Analyst, eCommerce analyst, ...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Seedsync Group of companies</td>\n",
       "      <td>Buy side stock analysts work with the fund man...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Gensuite LLC</td>\n",
       "      <td>Able to understand various data structures and...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Associate Data Analyst-Delivery &amp; Data Pull - ...</td>\n",
       "      <td>Spiceworks</td>\n",
       "      <td>Exposure to data visualization tools, preferab...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Steward Analyst</td>\n",
       "      <td>JLL</td>\n",
       "      <td>The role will have wide range of responsibilit...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Cornucopia, V5 Global</td>\n",
       "      <td>Expertise in data analytical tools and landsca...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Analyst - Referential data</td>\n",
       "      <td>Société Générale</td>\n",
       "      <td>Significant experience particularly in desktop...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Uplatz</td>\n",
       "      <td>Thoroughly understand the underlying data, bus...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst, Insight and Monitoring</td>\n",
       "      <td>Standard Chartered</td>\n",
       "      <td>To identify the appropriate data streams from ...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Process Associate - Data Analyst</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>To take ownership of all the assigned categori...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           JOB_TITLE  \\\n",
       "0                         Business Analyst (1-2 Yrs)   \n",
       "1                                       Data Analyst   \n",
       "2                                       Data Analyst   \n",
       "3  Associate Data Analyst-Delivery & Data Pull - ...   \n",
       "4                               Data Steward Analyst   \n",
       "5                                       Data Analyst   \n",
       "6                         Analyst - Referential data   \n",
       "7                                       Data Analyst   \n",
       "8               Data Analyst, Insight and Monitoring   \n",
       "9                   Process Associate - Data Analyst   \n",
       "\n",
       "                       COMPANY  \\\n",
       "0                   CommerceIQ   \n",
       "1  Seedsync Group of companies   \n",
       "2                 Gensuite LLC   \n",
       "3                   Spiceworks   \n",
       "4                          JLL   \n",
       "5        Cornucopia, V5 Global   \n",
       "6             Société Générale   \n",
       "7                       Uplatz   \n",
       "8           Standard Chartered   \n",
       "9                      Genpact   \n",
       "\n",
       "                                        EXPECTATIONS              LOCATION  \n",
       "0  data integration, Analyst, eCommerce analyst, ...  Bengaluru, Karnataka  \n",
       "1  Buy side stock analysts work with the fund man...  Bengaluru, Karnataka  \n",
       "2  Able to understand various data structures and...  Bengaluru, Karnataka  \n",
       "3  Exposure to data visualization tools, preferab...  Bengaluru, Karnataka  \n",
       "4  The role will have wide range of responsibilit...  Bengaluru, Karnataka  \n",
       "5  Expertise in data analytical tools and landsca...  Bengaluru, Karnataka  \n",
       "6  Significant experience particularly in desktop...  Bengaluru, Karnataka  \n",
       "7  Thoroughly understand the underlying data, bus...  Bengaluru, Karnataka  \n",
       "8  To identify the appropriate data streams from ...  Bengaluru, Karnataka  \n",
       "9  To take ownership of all the assigned categori...  Bengaluru, Karnataka  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "weather = pd.DataFrame({\n",
    "        \"JOB_TITLE\": job_name[:10],\n",
    "         \"COMPANY\": company[:10],\n",
    "         \"EXPECTATIONS\": summary[:10],\n",
    "         \"LOCATION\": job_location[:10]\n",
    "         \n",
    "    })\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text1 = requests.get(\"https://in.indeed.com/jobs?q=data%20scientist&l=Bangalore%2C%20Karnataka&advn=7641058883739924&vjk=6dc1a9b79905765e\").text\n",
    "soup1 = BeautifulSoup(html_text1,'html.parser')\n",
    "jobs1 = soup1.find_all('div',class_='jobsearch-SerpJobCard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = []\n",
    "Company = []\n",
    "Summary = []\n",
    "location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs1:\n",
    "    btag = i.h2.a\n",
    "    title = btag.get('title')\n",
    "    name.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in jobs1:\n",
    "    Ctag = x.find('span','company').text.strip()\n",
    "    Company.append(Ctag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pla in jobs1:\n",
    "    place = pla.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    location.append(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs1:\n",
    "    sam = i.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    Summary.append(sam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>EXPECTATIONS</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist (4-6 yrs)</td>\n",
       "      <td>CommerceIQ</td>\n",
       "      <td>Work with large scale ecommerce data of the bi...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist (NLP) for a Product Based Edute...</td>\n",
       "      <td>Shaw Academy</td>\n",
       "      <td>Prior experience in working with databases and...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Rinteger</td>\n",
       "      <td>(2)Processing, cleansing, and verifying the in...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist I</td>\n",
       "      <td>Honeywell</td>\n",
       "      <td>Deliver business value through Right and Fast ...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Arista Networks</td>\n",
       "      <td>Two years of experience in data science/AI tec...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Plotly Dash</td>\n",
       "      <td>loyalytics consulting</td>\n",
       "      <td>Experienced in web crawling and data scraping....</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Eton Solutions</td>\n",
       "      <td>About the Company ETON Solutions is a third ge...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Uplatz</td>\n",
       "      <td>Undertaking machine learning experiments and t...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist Jobs in India, Bengaluru...</td>\n",
       "      <td>Razorpay</td>\n",
       "      <td>We're seeking experienced data scientists to d...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WM Data Scientist</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>Advanced knowledge of data, application and in...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           JOB_TITLE  \\\n",
       "0                    Senior Data Scientist (4-6 yrs)   \n",
       "1  Data Scientist (NLP) for a Product Based Edute...   \n",
       "2                                     Data Scientist   \n",
       "3                                   Data Scientist I   \n",
       "4                                     Data Scientist   \n",
       "5                       Data Scientist - Plotly Dash   \n",
       "6                                     Data Scientist   \n",
       "7                                     Data Scientist   \n",
       "8  Senior Data Scientist Jobs in India, Bengaluru...   \n",
       "9                                  WM Data Scientist   \n",
       "\n",
       "                     COMPANY  \\\n",
       "0                 CommerceIQ   \n",
       "1               Shaw Academy   \n",
       "2                   Rinteger   \n",
       "3                  Honeywell   \n",
       "4            Arista Networks   \n",
       "5      loyalytics consulting   \n",
       "6             Eton Solutions   \n",
       "7                     Uplatz   \n",
       "8                   Razorpay   \n",
       "9  JPMorgan Chase Bank, N.A.   \n",
       "\n",
       "                                        EXPECTATIONS              LOCATION  \n",
       "0  Work with large scale ecommerce data of the bi...  Bengaluru, Karnataka  \n",
       "1  Prior experience in working with databases and...  Bengaluru, Karnataka  \n",
       "2  (2)Processing, cleansing, and verifying the in...  Bengaluru, Karnataka  \n",
       "3  Deliver business value through Right and Fast ...  Bengaluru, Karnataka  \n",
       "4  Two years of experience in data science/AI tec...  Bengaluru, Karnataka  \n",
       "5  Experienced in web crawling and data scraping....  Bengaluru, Karnataka  \n",
       "6  About the Company ETON Solutions is a third ge...  Bengaluru, Karnataka  \n",
       "7  Undertaking machine learning experiments and t...  Bengaluru, Karnataka  \n",
       "8  We're seeking experienced data scientists to d...  Bengaluru, Karnataka  \n",
       "9  Advanced knowledge of data, application and in...  Bengaluru, Karnataka  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "qwe = pd.DataFrame({\n",
    "        \"JOB_TITLE\": name[:10],\n",
    "         \"COMPANY\": Company[:10],\n",
    "         \"EXPECTATIONS\": Summary[:10],\n",
    "         \"LOCATION\": location[:10]\n",
    "    })\n",
    "qwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scrape data for “Data Scientist” designation for first 10 job results from Naukri.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text2 = requests.get(\"https://in.indeed.com/jobs?q=data%20scientist%20%E2%82%B93%2C00%2C000&l=Delhi&ts=1613473497941&rq=1&rsIdx=1\").text\n",
    "soup2 = BeautifulSoup(html_text2,'html.parser')\n",
    "jobs2 = soup2.find_all('div',class_='jobsearch-SerpJobCard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = []\n",
    "Company_name = []\n",
    "Summary = []\n",
    "Location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs2:\n",
    "    ytag = i.h2.a\n",
    "    title = ytag.get('title')\n",
    "    job_name.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in jobs2:\n",
    "    otag = x.find('span','company').text.strip()\n",
    "    Company_name.append(otag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pl in jobs2:\n",
    "    Place = pl.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    Location.append(Place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs2:\n",
    "    sym = i.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    Summary.append(sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>EXPECTATIONS</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Ohmyhome</td>\n",
       "      <td>Work with data and analytics experts to strive...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>Big data &amp; advanced analytics, Technology &amp; di...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>GreenTech Intelligent Transportation System LLP</td>\n",
       "      <td>Good programming skills in Python/C++. Experie...</td>\n",
       "      <td>Kalkaji, Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Ank Aha</td>\n",
       "      <td>Ensure data integrity and consistency across r...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ESRI, Inc.</td>\n",
       "      <td>As a data scientist, you will develop deep lea...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NLP &amp; Machine learning Developer</td>\n",
       "      <td>Kiiara Analytics Pvt Ltd.</td>\n",
       "      <td>Proficient in data scraping. Candidate have to...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Contractual Role-Data Science (3 months)</td>\n",
       "      <td>Edelman</td>\n",
       "      <td>Strong communication skills to translate data ...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sr.Data Scientist</td>\n",
       "      <td>Innefu Labs Pvt. Ltd.</td>\n",
       "      <td>Must be able to handle deployment complexities...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science &amp; Analytic</td>\n",
       "      <td>CODEC Networks</td>\n",
       "      <td>1 to 2 years developing and implementing data ...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Manager Data Scientist</td>\n",
       "      <td>Innefu Labs Pvt. Ltd.</td>\n",
       "      <td>Must be able to handle deployment complexities...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  JOB_TITLE  \\\n",
       "0                             Data Engineer   \n",
       "1                            Data Scientist   \n",
       "2                     Senior Data Scientist   \n",
       "3                            Data Scientist   \n",
       "4                            Data Scientist   \n",
       "5          NLP & Machine learning Developer   \n",
       "6  Contractual Role-Data Science (3 months)   \n",
       "7                         Sr.Data Scientist   \n",
       "8                   Data Science & Analytic   \n",
       "9                    Manager Data Scientist   \n",
       "\n",
       "                                           COMPANY  \\\n",
       "0                                         Ohmyhome   \n",
       "1                          Boston Consulting Group   \n",
       "2  GreenTech Intelligent Transportation System LLP   \n",
       "3                                          Ank Aha   \n",
       "4                                       ESRI, Inc.   \n",
       "5                        Kiiara Analytics Pvt Ltd.   \n",
       "6                                          Edelman   \n",
       "7                            Innefu Labs Pvt. Ltd.   \n",
       "8                                   CODEC Networks   \n",
       "9                            Innefu Labs Pvt. Ltd.   \n",
       "\n",
       "                                        EXPECTATIONS               LOCATION  \n",
       "0  Work with data and analytics experts to strive...           Delhi, Delhi  \n",
       "1  Big data & advanced analytics, Technology & di...       New Delhi, Delhi  \n",
       "2  Good programming skills in Python/C++. Experie...  Kalkaji, Delhi, Delhi  \n",
       "3  Ensure data integrity and consistency across r...       New Delhi, Delhi  \n",
       "4  As a data scientist, you will develop deep lea...       New Delhi, Delhi  \n",
       "5  Proficient in data scraping. Candidate have to...       New Delhi, Delhi  \n",
       "6  Strong communication skills to translate data ...           Delhi, Delhi  \n",
       "7  Must be able to handle deployment complexities...           Delhi, Delhi  \n",
       "8  1 to 2 years developing and implementing data ...           Delhi, Delhi  \n",
       "9  Must be able to handle deployment complexities...           Delhi, Delhi  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "twe = pd.DataFrame({\n",
    "        \"JOB_TITLE\": job_name[:10],\n",
    "         \"COMPANY\": Company_name[:10],\n",
    "         \"EXPECTATIONS\": Summary[:10],\n",
    "         \"LOCATION\": Location[:10]\n",
    "    })\n",
    "twe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    "Brand Product Description Price Discount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand1 = []\n",
    "Product_Description1 = []\n",
    "Price1 = []\n",
    "Discount1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pags = list(range(1,5))\n",
    "for page in pags:\n",
    "    requr = requests.get(\"https://www.flipkart.com/search?q=sunglasses&sid=26x&as=on&as-show=on&otracker=AS_QueryStore_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_QueryStore_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sunglasses%7CSunglasses&requestId=ed6f54e5-30b5-43c9-80e9-476306844180&as-searchtext=sung\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup9 = BeautifulSoup(requr,'html.parser')\n",
    "    des = soup9.find_all('div' , class_='_2WkVRV')\n",
    "    for i in range(len(des)):\n",
    "        Brand1.append(des[i].text)\n",
    "    len(Brand1)\n",
    "    price = soup9.find_all('div',class_='_30jeq3') \n",
    "    for i in range(len(price)):\n",
    "        Price1.append(price[i].text)\n",
    "        len(Price1)\n",
    "    xc = soup9.find_all('a',class_='IRpwTa') \n",
    "    for i in range(len(xc)):\n",
    "        Product_Description1.append(xc[i].text)\n",
    "        len(xc)\n",
    "    cv = soup9.find_all('div',class_='_3Ay6Sb')\n",
    "    for i in range(len(cv)):\n",
    "        Discount1.append(cv[i].text)\n",
    "        len(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESC</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized Retro Square Sunglasses (52)</td>\n",
       "      <td>₹509</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>Mirrored, UV Protection, Riding Glasses, Other...</td>\n",
       "      <td>₹287</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹683</td>\n",
       "      <td>24% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹187</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹469</td>\n",
       "      <td>53% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phenomenal</td>\n",
       "      <td>UV Protection, Mirrored Retro Square Sunglasse...</td>\n",
       "      <td>₹570</td>\n",
       "      <td>28% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>UV Protection, Polarized Aviator Sunglasses (F...</td>\n",
       "      <td>₹233</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (58)</td>\n",
       "      <td>₹296</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (58)</td>\n",
       "      <td>₹685</td>\n",
       "      <td>23% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Mirrored, Gradient Aviator, Way...</td>\n",
       "      <td>₹212</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           BRAND                                       PRODUCT_DESC PRICE  \\\n",
       "0      ROYAL SON             Polarized Retro Square Sunglasses (52)  ₹509   \n",
       "1      New Specs  Mirrored, UV Protection, Riding Glasses, Other...  ₹287   \n",
       "2       Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹683   \n",
       "3         PIRASO              UV Protection Aviator Sunglasses (54)  ₹187   \n",
       "4       Fastrack  Mirrored, UV Protection Wayfarer Sunglasses (F...  ₹469   \n",
       "..           ...                                                ...   ...   \n",
       "95    phenomenal  UV Protection, Mirrored Retro Square Sunglasse...  ₹570   \n",
       "96  Singco India  UV Protection, Polarized Aviator Sunglasses (F...  ₹233   \n",
       "97      Fastrack              UV Protection Aviator Sunglasses (58)  ₹296   \n",
       "98      Fastrack              UV Protection Aviator Sunglasses (58)  ₹685   \n",
       "99         NuVew  UV Protection, Mirrored, Gradient Aviator, Way...  ₹212   \n",
       "\n",
       "   DISCOUNT  \n",
       "0   66% off  \n",
       "1   79% off  \n",
       "2   24% off  \n",
       "3   88% off  \n",
       "4   53% off  \n",
       "..      ...  \n",
       "95  28% off  \n",
       "96  84% off  \n",
       "97  81% off  \n",
       "98  23% off  \n",
       "99  86% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "kwe = pd.DataFrame({\n",
    "        \"BRAND\": Brand1[:100],\n",
    "         \"PRODUCT_DESC\": Product_Description1[:100],\n",
    "         \"PRICE\": Price1[:100],\n",
    "         \"DISCOUNT\": Discount1[:100]\n",
    "    })\n",
    "kwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.¶ Scrape 100 reviews data from flipkart.com for iphone11\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Rating = []\n",
    "Review = []\n",
    "Review_Summary = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pag = list(range(1,12))\n",
    "for page in pag:\n",
    "    reque = requests.get(\"https://www.flipkart.com/apple-iphone-11-white-64-gb/product-reviews/itmfc6a7091eb20b?pid=MOBFWQ6BVWVEH3XE&lid=LSTMOBFWQ6BVWVEH3XESAHPTP&marketplace=FLIPKART\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup6 = BeautifulSoup(reque,'html.parser')\n",
    "    qwer = soup6.find_all('div' , class_='_3LWZlK _1BLPMq')\n",
    "    for i in range(len(qwer)):\n",
    "        Rating.append(qwer[i].text)\n",
    "    len(Rating)\n",
    "    rev = soup6.find_all('p',class_='_2-N8zT') \n",
    "    for i in range(len(rev)):\n",
    "        Review.append(rev[i].text)\n",
    "        len(Review)\n",
    "    rs = soup6.find_all('div',class_='t-ZTKy') \n",
    "    for i in range(len(rs)):\n",
    "        Review_Summary.append(rs[i].text)\n",
    "        len(rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATING</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW_SUMMARY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the MoneyThe iPhone 11 offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.I’m am ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️Its awesome mobile phone in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>*Review after 10 months of usage*Doesn't seem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Best budget Iphone till date ❤️ go for it guys...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RATING              REVIEW  \\\n",
       "0       5           Brilliant   \n",
       "1       5    Perfect product!   \n",
       "2       5   Worth every penny   \n",
       "3       5       Great product   \n",
       "4       5  Highly recommended   \n",
       "..    ...                 ...   \n",
       "95      5    Perfect product!   \n",
       "96      5    Perfect product!   \n",
       "97      5           Wonderful   \n",
       "98      5           Fabulous!   \n",
       "99      5   Worth every penny   \n",
       "\n",
       "                                       REVIEW_SUMMARY  \n",
       "0   The Best Phone for the MoneyThe iPhone 11 offe...  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Previously I was using one plus 3t it was a gr...  \n",
       "3   Amazing Powerful and Durable Gadget.I’m am ver...  \n",
       "4   iphone 11 is a very good phone to buy only if ...  \n",
       "..                                                ...  \n",
       "95  It’s a must buy who is looking for an upgrade ...  \n",
       "96  Value for money❤️❤️Its awesome mobile phone in...  \n",
       "97  *Review after 10 months of usage*Doesn't seem ...  \n",
       "98  This is my first iOS phone. I am very happy wi...  \n",
       "99  Best budget Iphone till date ❤️ go for it guys...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pwe = pd.DataFrame({\n",
    "        \"RATING\": Rating[:100],\n",
    "         \"REVIEW\": Review[:100],\n",
    "         \"REVIEW_SUMMARY\": Review_Summary[:100],\n",
    "    })\n",
    "pwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Scrape data for first 100 sneakers you find when you visit flipkart.com.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "discount = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pags = list(range(1,5))\n",
    "for page in pags:\n",
    "    reqe = requests.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup5 = BeautifulSoup(reqe,'html.parser')\n",
    "    descr = soup5.find_all('div' , class_='_2WkVRV')\n",
    "    for i in range(len(descr)):\n",
    "        Brand.append(descr[i].text)\n",
    "    len(Brand)\n",
    "    price = soup5.find_all('div',class_='_30jeq3') \n",
    "    for i in range(len(price)):\n",
    "        Price.append(price[i].text)\n",
    "        len(Price)\n",
    "    xc = soup5.find_all('a',class_='IRpwTa') \n",
    "    for i in range(len(xc)):\n",
    "        Product_Description.append(xc[i].text)\n",
    "        len(xc)\n",
    "    cv = soup5.find_all('div',class_='_3Ay6Sb')\n",
    "    for i in range(len(cv)):\n",
    "        discount.append(cv[i].text)\n",
    "        len(cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESC</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French Connection</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹680</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"trend\"</td>\n",
       "      <td>Fashionable casual sneakers shoes for men Snea...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Perfect &amp; Affordable Combo Pack of 02 Pairs Sn...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹379</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Super &amp; Trendy Men's Pack of 02 Pair Shoes for...</td>\n",
       "      <td>₹420</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>believe</td>\n",
       "      <td>Sneakers for men(black_6) Sneakers For Men</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>richerson</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹398</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>494 Perfect Sports Shoes for Running Training ...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>India hub</td>\n",
       "      <td>high quality casual sneaker shoes Sneakers For...</td>\n",
       "      <td>₹349</td>\n",
       "      <td>67% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Urban Plus CV Sneakers For Men</td>\n",
       "      <td>₹298</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                BRAND                                       PRODUCT_DESC  \\\n",
       "0   French Connection                                   Sneakers For Men   \n",
       "1             \"trend\"  Fashionable casual sneakers shoes for men Snea...   \n",
       "2              Chevit  Perfect & Affordable Combo Pack of 02 Pairs Sn...   \n",
       "3        Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "4              Chevit  Super & Trendy Men's Pack of 02 Pair Shoes for...   \n",
       "..                ...                                                ...   \n",
       "95            believe         Sneakers for men(black_6) Sneakers For Men   \n",
       "96          richerson                                   Sneakers For Men   \n",
       "97             Chevit  494 Perfect Sports Shoes for Running Training ...   \n",
       "98          India hub  high quality casual sneaker shoes Sneakers For...   \n",
       "99               PUMA                     Urban Plus CV Sneakers For Men   \n",
       "\n",
       "   PRICE DISCOUNT  \n",
       "0   ₹680  65% off  \n",
       "1   ₹499  87% off  \n",
       "2   ₹499  72% off  \n",
       "3   ₹379  62% off  \n",
       "4   ₹420  78% off  \n",
       "..   ...      ...  \n",
       "95  ₹474  76% off  \n",
       "96  ₹398  65% off  \n",
       "97  ₹474  70% off  \n",
       "98  ₹349  67% off  \n",
       "99  ₹298  62% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "awe = pd.DataFrame({\n",
    "        \"BRAND\": Brand[:100],\n",
    "         \"PRODUCT_DESC\": Product_Description[:100],\n",
    "         \"PRICE\": Price[:100],\n",
    "         \"DISCOUNT\": discount[:100]\n",
    "    })\n",
    "awe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Scrape data for first 10 Laptops you find when you visit flipkart.com.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "descriptions = []\n",
    "prices = []\n",
    "ratings = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list(range(1,2))\n",
    "for page in pages:\n",
    "    req = requests.get(\"https://www.flipkart.com/search?q=laptops&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={}\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup4 = BeautifulSoup(req,'html.parser')\n",
    "    desc = soup4.find_all('div' , class_='_4rR01T')\n",
    "    for i in range(len(desc)):\n",
    "        descriptions.append(desc[i].text)\n",
    "    len(descriptions)\n",
    "    price = soup4.find_all('div',class_='_30jeq3 _1_WHN1') \n",
    "    for i in range(len(price)):\n",
    "        prices.append(price[i].text)\n",
    "        len(prices)\n",
    "\n",
    "    rating = soup4.find_all('div',class_='_3LWZlK') \n",
    "    for i in range(len(rating)):\n",
    "        ratings.append(rating[i].text)\n",
    "        len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>RATING</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo Ideapad Slim 3 Celeron Dual Core - (4 G...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>₹23,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP 15s Dual Core 3020e - (4 GB/1 TB HDD/Window...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>₹22,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acer Aspire 7 Ryzen 5 Hexa Core 5500U - (8 GB/...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>₹54,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lenovo Ideapad Slim 3i Core i3 10th Gen - (8 G...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹35,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>msi GF63 Thin Core i5 9th Gen - (8 GB/512 GB S...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>₹52,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS ExpertBook P1 Core i3 10th Gen - (4 GB/1 ...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>₹26,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>acer One 14 Pentium Gold - (4 GB/1 TB HDD/Wind...</td>\n",
       "      <td>4</td>\n",
       "      <td>₹19,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lenovo Ideapad S145 Ryzen 3 Dual Core 3200U - ...</td>\n",
       "      <td>4</td>\n",
       "      <td>₹26,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS Celeron Dual Core - (4 GB/1 TB HDD/Window...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>₹19,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo Ideapad Gaming 3 Ryzen 5 Hexa Core 4600...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>₹59,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE RATING    PRICE\n",
       "0  Lenovo Ideapad Slim 3 Celeron Dual Core - (4 G...    4.2  ₹23,990\n",
       "1  HP 15s Dual Core 3020e - (4 GB/1 TB HDD/Window...    3.9  ₹22,990\n",
       "2  acer Aspire 7 Ryzen 5 Hexa Core 5500U - (8 GB/...    4.5  ₹54,990\n",
       "3  Lenovo Ideapad Slim 3i Core i3 10th Gen - (8 G...    4.3  ₹35,990\n",
       "4  msi GF63 Thin Core i5 9th Gen - (8 GB/512 GB S...    4.5  ₹52,990\n",
       "5  ASUS ExpertBook P1 Core i3 10th Gen - (4 GB/1 ...    3.8  ₹26,490\n",
       "6  acer One 14 Pentium Gold - (4 GB/1 TB HDD/Wind...      4  ₹19,990\n",
       "7  Lenovo Ideapad S145 Ryzen 3 Dual Core 3200U - ...      4  ₹26,990\n",
       "8  ASUS Celeron Dual Core - (4 GB/1 TB HDD/Window...    3.9  ₹19,990\n",
       "9  Lenovo Ideapad Gaming 3 Ryzen 5 Hexa Core 4600...    4.5  ₹59,990"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dwe = pd.DataFrame({\n",
    "        \"TITLE\": descriptions[:10],\n",
    "         \"RATING\": ratings[:10],\n",
    "         \"PRICE\": prices[:10],\n",
    "    })\n",
    "dwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
